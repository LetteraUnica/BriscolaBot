\chapter{Background}
When an human first plays a game, let it be chess, checkers, poker, minecraft, cs:go or any other game, it is not given any instructions on how to play. It is not told which moves are good and which are bad, maybe it doesn't even know the rules of the game. By interacting and playing the game, losing to some opponents and starting to beat others, the player gradually learns what moves are best in which situation, and starts to understand the dynamics of the game. During all of this the player doesn't have any external feedback telling him which actions to perform in a given situation, but has to learn it by itself, by performing the action and seeing the outcome. Learning from interaction is a foundational idea underlying nearly all theories of learning and intelligence \cite{sutton-barto}.

In this chapter we provide an overview of reinforcement learning. First of all we explore markov decision processes (MPDs) that allow us to define an environment in mathematical terms, ... Our dissertion of reinforcement learning is based on the book \cite{sutton-barto}..

\section{Reinforcement Learning}
Reinforcement learning is the field of machine learning that tries to maximize a numerical reward signal. The agent is not said which actions to take, but instead must discover the actions that lead to the highest reward by trying them. Furthermore, actions do not only influence the immediate future reward but also the next situation and consequently subsequent rewards, so the agent must learn to balance immediate and future rewards.\\\\
Reinforcement learning differs from supervised learning because there is no external supervisor, which tells the agent which actions to perform in a given situation, instead the agent must discover the actions that lead to the highest reward by trying them. 
Reinforcement learning is also different from unsupervised learning, because even though both approaches do not need labels to learn from, their goal is different. In unsupervised learning the goal is to find structure in the data, while in reinforcement learning the goal is to maximize a reward signal.\\\\
One important aspect of reinforcement learning is the exploration-exploitation trade-off, which is not present in supervised or unsupervised learning. The agent must weigh the benefits of exploring new actions against the benefits of utilizing the best actions it has already discovered. A focus on exploration alone may lead to the discovery of the best actions, but with infrequent use, while a focus on exploitation alone may prevent the discovery of potentially better actions.

\section{Markov Decision Processes}

We can formalize the problem of learning from interaction to achieve a goal with Markov Decision Processes (MDPs). In the MPD framework the learner is called the agent, and the thing it interacts with is called the environment. The agent continually interacts with the environment by performing actions, on each action the environment changes state, and presents a new situation to the agent. The environment is also responsible to give the reward signal the agent must maximize.\\\\
More specifically, the agent and environment interact in discrete time steps ($t=0, 1, 2 \dots$). At each time step $t$ the agent receives an observation of the environment's state $S_t \in S$ and based on that chooses an action $A_t \in A$. In the next time step $t+1$ the agent receives from the environment a numerical reward $R_{t+1} \in R \in \mathbb{R}$ and a new observation $S_{t+1} \in S$ and so on. This generates a sequence of observations, actions and rewards, which we call trajectory: $S_0, A_0, R_1, S_1, A_1, R_2, \dots$.
\begin{figure}[H]
    \centering
    \includesvg[width=0.6\textwidth]{images/mdp.svg}
    \caption{Illustration of the agent-environment interaction in a Markov Decision Process}
    \label{fig:mdp}
\end{figure}
\clearpage
In finite MDPs the set of states $S$, actions $A$ and rewards $R$ are finite, so the distribution of next state and reward, given the current state and action is well defined. Furthermore, the distribution of the random variables $R_t$ and $S_t$ follows the Markov property, depending only on the previous state and action, $P(S_{t+1}|S_t) = P(S_{t+1}|S_t, S_{t-1}, \dots, S_0)$.\\\\
We can formalize mathematically an MDP using the following notation:
\begin{itemize}
    \item $S$ is the set of states
    \item $A$ is the set of actions
    \item $R$ is the set of rewards
    \item $p(s', r | s, a)$ is the dynamics function, which describes the distribution of the next state $s'$ and reward $r$, given the current state $s$ and action $a$, defined as:
    \begin{equation}
        p(s', r | s, a) = \mathds{P}[S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a]
        \label{dynamics-function}
    \end{equation}
    \item $\gamma \in [0, 1]$ is the discount factor, which determines how much importance the agent gives to future rewards
\end{itemize}

\subsection{Reward}
The goal of the agent is formalized via a reward signal, given from the environment to the agent. At each time step the reward is a single number $R_t \in \mathbb{R}$. The agent goal is to maximize the total reward it receives over time.\\
For example if we want the agent to play a game, the reward signal can be the number of points the agent has scored. If we want the agent to learn to walk, the reward signal can be the distance the agent has traveled.\\
The reward signal is not a way of telling the agent how to do, if we were to do that the agent will accomplish side tasks without ever completing the main task that we want to be solved. For example in a racing game we might be tempted to give a reward whenever the car moves forward, but this will lead the agent to move forward and backward in a loop, without ever finishing the race. In fact, in this case finishing the race is the worst thing that could happen, because then the agent will stop receiving the reward signal.\\
The reward signal thus specifies what we want the agent to do, not how to do it.

\subsection{Episodic and Continuing Tasks}
We can divide the kinds of problems that reinforcement learning deals with in two classes: episodic problems and continuing ones, as we will see both of them can be well formalized with MDPs.\\
In episodic tasks the agent interacts with the environment for a finite (yet random) number of time steps $T$, and then it reaches a terminal state, ending the episode. Examples of episodic tasks is the game of chess, as the episode ends when a player wins or a draw is made.\\
In continuing tasks, on the other hand, there is no clear notion of episode, as the agent interacts with the environment continually, potentially for an infinite number of time steps.\\
We can formalize both problems with MDPs, by defining the concept of terminal state: a terminal state is an absorbing state and corresponds to the end of the episode, when the agent reaches the terminal state it can no longer escape it and all future rewards are zero.\\

\begin{figure}
    \centering
    \includesvg[width=0.6\textwidth]{images/episodic-continuing.svg}
    \caption{Empty circles represen the states, while solid circles the actions state-action pairs. The numbers indicate the reward the agent receives if it goes through the corresponding transition. Left: Illustration of an episodic task, the terminal state $s_T$ is represented with a square. If the agent reaches the terminal state it can no longer receive any reward. Right: Illustration of a continuing task, when the agent reaches $s_2$ it comes back to $s_1$, receiving a reward of $r=1$ in the process, in this MPD the expected return with $\gamma = 1$ would be infinite as the sequence of rewards after the first state is always $1$.}
    \label{fig:episodic-continuing}
\end{figure}

\subsection{Goal and Return}
In reinforcement learning we formalize the goal of an agent as the maximization of the expected return $G_t$. The expected return is the sum of all rewards the agent receives in the future, starting from a given time step $t$:

\begin{equation}
    G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
    \label{return}
\end{equation}

$\gamma \in [0, 1]$ is the discount factor and is used for two main reasons:
\begin{enumerate}
    \item $\gamma$ allows to tune how much importance the agent gives to future rewards, so we can make the agent more or less patient. If $\gamma = 0$ the agent will only consider the immediate reward $R_{t+1}$, while if $\gamma = 1$ the agent will consider all future rewards with the same importance.
    \item Allows the return to be used also in continuing tasks, because we can set $\gamma < 1$ and keep the sum from diverging.
\end{enumerate}
Returns of subsequent time steps are related to each other in an important way:
\begin{equation}
    G_t = R_{t+1} + \gamma G_{t+1}
    \label{return-recursive}
\end{equation}

\subsection{Policy and value functions}
A policy $\pi$ is a mapping from states to actions, which specifies what action the agent should take in a given state. The policy is formally defined as the probability of taking action $a$ in state $s$
\begin{equation}
    \pi(a | s) = \mathds{P}[A_t = a | S_t = s]
    \label{policy}
\end{equation}
Given a policy we can define a value function $v_{\pi}(s)$ associated to the policy, which gives us a measure of how good it is for the agent to be in a given state when following policy $\pi$. The value function is defined mathematically as the expected return starting from a state $s$ and then following policy $\pi$:
\begin{equation}
    v_{\pi}(s) = \mathds{E_{\pi}}[G_t | S_t = s] = \mathds{E_{\pi}}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s]
    \label{value-function}
\end{equation}
We denote with $v_{\pi}(s)$ the value function of policy $\pi$.\\\\
In a similar manner we can define the value of selecting an action $a$ in state $s$ under policy $\pi$, denoted $q_{\pi}(s, a)$, as the expected return starting from $s$, taking the action $a$ and then following policy $\pi$:
\begin{equation}
    q_{\pi}(s, a) = \mathds{E_{\pi}}[G_t | S_t = s, A_t = a] = \mathds{E_{\pi}}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a]
    \label{action-value-function}
\end{equation}
We call $q_{\pi}(s, a)$ the action-value function of policy $\pi$.\\\\
In practice it is also convenient to define the advantage, which is the difference between the value of taking action $a$ in state $s$ and the value of the state:
\begin{equation}
    A(s, a) = q_{\pi}(s, a) - v_{\pi}(s)
    \label{advantage}
\end{equation}
The advantage is a measure of how much better it is to take action $a$ in state $s$.\\

\subsection{Bellman Equation}
A fundamental property of the value function is that it satisfies a recursive relationship similar to the one of the return \ref{return-recursive}:
\begin{equation}
    \begin{split}
        v_{\pi}(s) & = \mathds{E_{\pi}}[G_t | S_t = s] \\
        & = \mathds{E_{\pi}}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
        & = \sum_a \pi(a | s) \sum_{s', r} p(s', r | s, a) (r + \gamma v_{\pi}(s'))
    \end{split}
    \label{bellman-equation}
\end{equation}
We call equation \ref{bellman-equation} the Bellman equation for $v_{\pi}$. The Bellman equation expresses the relationship between the value of current state $s$ and the value of successive states $s'$. It tells us that the value of each state must equal the value of the next state plus the reward expected along the way. The Bellman equation forms the basis to compute or approximate $v_{\pi}$ which we will see in the next  section.\\\\
It is also possible to define the Bellman equation for the action-value function:
\begin{equation}
    \begin{split}
        q_{\pi}(s, a) & = \mathds{E}[G_t | S_t = s, A_t = a] \\
        & = \mathds{E}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] \\
        & = \sum_{s', r} p(s', r | s, a) (r + \gamma v(s')) \\
        & = \sum_{s', r} p(s', r | s, a) (r + \gamma \sum_{a'} \pi(a' | s') q_{\pi}(s', a'))
    \end{split}
    \label{bellman-equation-action-value}
\end{equation}
Where we used the relationship $v_{\pi}(s) = \sum_a \pi(a | s) q_{\pi}(s, a)$ between the value function and action-value function.\\\\

\subsection{Optimal policy and value function}
Solving a reinforcement learning problem means finding a policy that achieves a high return. To accomplish this we first need to define an ordering over policies. We define that policy $\pi$ is better than policy $\pi'$ if the expected return is higher for $\pi$ than for $\pi'$.
\begin{equation}
    \pi \geq \pi' \iff v_{\pi}(s) \geq v_{\pi'}(s) \quad \forall s \in S
    \label{policy-ordering}
\end{equation}
This way we can define the optimal policy as the policy that is at least better or equal to all other policies.
\begin{equation}
    \pi^* \geq \pi' \quad \forall \pi'
    \label{optimal-policy}
\end{equation}
Although there may be more than one optimal policy, all of them will have the same value-function $v_*$, so we can denote all the optimal policies with $\pi^*$.
We define the optimal state-value function $v_*$ as:
\begin{equation}
    v_*(s) = \max_{\pi} v_{\pi}(s) \quad \forall s \in S
    \label{optimal-value-function}
\end{equation}
Which is the value function that achieves the maximum return from all states $s$. We can also define the optimal action-value function $q_*$ as:
\begin{equation}
    q_*(s, a) = \max_{\pi} q_{\pi}(s, a) \quad \forall s \in S, \forall a \in A
    \label{optimal-action-value-function}
\end{equation}
This function gives the expected return for taking action $a$ in state $s$ and then following policy $\pi^*$.\\\\
Another property of the optimal state-value function is that it satisfies a special form of the Bellman equation \ref{bellman-equation}, called the Bellman optimality equation:
\begin{equation}
    \begin{split}
        v_{*}(s) & = \max_a q_*(s, a) \\
        & = \max_a \mathds{E_{\pi^*}}[G_t | S_t = s, A_t = a] \\
        & = \max_a \sum_{s', r} p(s', r | s, a) (r + \gamma v_*(s'))
    \end{split}
    \label{bellman-optimality-equation}
\end{equation}
Note that in the final statement we didn's specify any policy, this is because we assume to be following an optimal policy, which is the one that always selects the best action $a_* = \arg \max_a' q_*(s, a') \; \forall s \in S$.
Analogously, we can define the Bellman optimality equation for the action-value function:
\begin{equation}
    \begin{split}
        q_{*}(s, a) & = \mathds{E_{\pi^*}}[G_t | S_t = s, A_t = a] \\
        & = \sum_{s', r} p(s', r | s, a) (r + \gamma \max_{a'} q_*(s', a'))
    \end{split}
    \label{bellman-optimality-equation-action-value}
\end{equation}
Intuitively, the Bellman optimality equation expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state.\\
In the next sections we will see how to solve the Bellman optimality equation both when we have a model of the environment \ref{dynamics-function} and when we don't.

\section{Dynamic Programming}
Dynamic programming is a tecnique that allows us to solve the Bellman optimality equation \ref{bellman-optimality-equation}, finding the optimal policy $\pi_*$ and value function $v_*$ given a perfect model of the environment, that is, we completely know the dynamics function $p(s', r | s, a)$.
We can divide the problem of finding a solution to the Bellman optimality equation into two subproblems:
\begin{enumerate}
    \item \textbf{Policy evaluation}: Given a policy $\pi$, find its value function $v_{\pi}$.
    \item \textbf{Policy improvement}: Given a value function $v_{\pi}$, find a better policy $\pi'$.
\end{enumerate}

\subsection{Policy evaluation}
A simple way to evaluate a policy $\pi$ would be to solve the linear system of equations described by the Bellman equation \ref{bellman-equation}. This could be done with any linear system solution method available in the literature, however, in reinforcement learning iterative solution methods are most suitable.
\begin{algorithm}[H]
    \caption{Policy evaluation}
    \label{alg:iterative-policy-evaluation}
    \begin{algorithmic}[1]
        \Require A policy $\pi$, a model $p$, a discount factor $\gamma$, a threshold $\theta$
        \State Initialize a candidate value function $\hat{v}_{\pi}$
        \While{True}
            \State $\hat{v}'_{\pi} = \hat{v}_{\pi}$
            \For{$s \in S$}
                \State $\hat{v}'_{\pi}(s) = \sum_a \pi(a | s) \sum_{s', r} p(s', r | s, a) (r + \gamma \hat{v}_{\pi}(s'))$
            \EndFor
            \If{$| \hat{v}'_{\pi}(s) - \hat{v}_{\pi}(s) | < \theta$}
                \State \textbf{return} $\hat{v}_{\pi}$
            \EndIf
        \EndWhile
    \end{algorithmic}
\end{algorithm}
Algorithm \ref{alg:iterative-policy-evaluation} is called the itearive policy evaluation algorithm and is guaranteed to always converge to the true value function $v_{\pi}$ with an error below the threshold $\theta$ if $\gamma < 1$, or if $\gamma = 1$ only if termination is guaranteed from all states under the policy $\pi$.\\
This can be proven mathematically by noting that the operator $B_{\pi}(v_{\pi}) = \sum_a \pi(a | s) \sum_{s', r} p(s', r | s, a) (r + \gamma v_{\pi}(s'))$ is contracting, which means that the solution is both unique and is reached no matter how we initialize the value function $\hat{v}_{\pi}$.\\\\
In algorithm \ref{alg:iterative-policy-evaluation} we used an additional array to store the updated value function $\hat{v}'_{\pi}$, however, this is not required, as the algorithm will converge regardless of how we perform the updates, that is, we could have used only one array $\hat{v}_{\pi}$ and performed the update on line $5$ in-place:\\
$\hat{v}_{\pi}(s) = \sum_a \pi(a | s) \sum_{s', r} p(s', r | s, a) (r + \gamma \hat{v}_{\pi}(s'))$\\
In practice, the in-place algorithm usually converges faster than the two-array version because it uses updated values as soon as they are available.

\subsection{Policy improvement}
Our reason for computing the value function for a policy is to help find better policies. A simple improved version of $\pi$ is a policy that in state $s'$ takes the action that maximizes $q_{\pi}(s', a)$. This policy will be equal to $\pi$ in all states $s \neq s'$ but will be better in state $s'$, so $\pi' \geq \pi$.\\
We can take this idea further, by defining $\pi'$ as the policy that takes the greedy action with respect to $q_{\pi}(s, a)$ in all states, with ties broken evenly. This policy is called the greedy policy and is defined as:
\begin{equation}
    \pi'(a | s) = \arg \max_a q_{\pi}(s, a) = \arg \max_a \sum_{s', r} p(s', r | s, a) (r + \gamma v_{\pi}(s'))
    \label{greedy-policy}
\end{equation}
The greedy policy always takes the action that looks best in the short term with just one step lookahead, according to $v_{\pi}$. It can be shown that the greedy policy is always better or equal to $\pi$ due to the policy improvement theorem, demonstrated on page 78 of the Sutton and Barto's book \cite{sutton-barto}.\\

\subsection{Policy iteration}
A simple way to find the optimal policy is to iterate the policy evaluation and policy improvement steps until the policy doesn't change anymore. This process is guaranteed to converge to the optimal policy and value function with the same conditions that apply to the policy evaluation algorithm \ref{alg:iterative-policy-evaluation}, that is: convergence is guaranteed to the true value function $v_{\pi}$ with an error below the threshold $\theta$ if $\gamma < 1$, or if $\gamma = 1$ only if the task is episodic.\\
\begin{algorithm}[H]
    \caption{Policy iteration}
    \label{alg:policy-iteration}
    \begin{algorithmic}[1]
        \Require A model $p$, a discount factor $\gamma$, a threshold $\theta$
        \State Initialize a policy $\pi$
        \While{True}
            \State $v_{\pi} = \mathrm{Policy \; evaluation} (\pi, p, \gamma, \theta)$ \Comment{Algorithm \ref{alg:iterative-policy-evaluation}}
            \State $\pi' = \arg \max_a \sum_{s', r} p(s', r | s, a) (r + \gamma v_{\pi}(s'))$ \Comment{Policy improvement}
            \If{$\pi' = \pi$}
                \State \textbf{return} $\pi, v_{\pi}$
            \EndIf
        \EndWhile
    \end{algorithmic}
\end{algorithm}
Algorithm \ref{alg:policy-iteration} is called the policy iteration algorithm. The algorithm iterates the policy evaluation and policy improvement steps until the policy doesn't change anymore.

\subsection{Value iteration}
The drawback of the policy iteration algorithm is that it requires an entire policy evaluation for each policy improvement step, requiring multiple sweeps over the state space. In practice, however, we can stop  before policy evalulation finished without losing any of the convergence properties of policy iteration \ref{alg:policy-iteration}. One important special case is when policy evaluation is stopped after just one sweep, using the update:
\begin{equation}
    v_{k+1}(s) = \max_a \sum_{s', r} p(s', r | s, a) (r + \gamma v_k(s'))
    \label{value-iteration-update}
\end{equation}
This update the value iteration update and combines one pass of policy evaluation with policy improvement into one, requiring only 1 sweep over the state space. The update \ref{value-iteration-update} is similar to the policy evaluation update \ref{bellman-equation} except that we are using the maximum instead of the expectation.
\begin{algorithm}[H]
    \caption{Value iteration}
    \label{alg:value-iteration}
    \begin{algorithmic}[1]
        \Require A model $p$, a discount factor $\gamma$, a threshold $\theta$
        \State Initialize a value function $v_{0}$
        \While{True}
            \State $v_{k+1} = v_k$
            \For{$s \in S$}
                \State $v_{k+1}(s) = \max_a \sum_{s', r} p(s', r | s, a) (r + \gamma v_k(s'))$
            \EndFor
            \If{$| v_{k+1}(s) - v_k(s) | < \theta$}
                \State \textbf{return} $v_{k+1}$
            \EndIf
        \EndWhile
    \end{algorithmic}
\end{algorithm}
Algorithm \ref{alg:value-iteration} is called the value iteration algorithm and usually converges faster than performing policy evaluation \ref{alg:iterative-policy-evaluation} at each step. Furthermore, like the policy evaluation algorithm \ref{alg:iterative-policy-evaluation}, this algorithm converges even if we perform all updates in place, without keeping in memory both $v_{k+1}$ and $v_k$, thus reducing memory requirements, on top of that .
