\chapter{Policy Gradient Methods}
So far we focused on action-value methods, those methods learned the action-value function $q$ and used it to derive a policy. In this chapter, we will focus on methods that learn the policy directly, without consulting a value, or action-value function. The value function still has an important role as it's often used to learn the policy, however, it is not required for action selection. This class of methods is called policy gradient methods.\\\\
Policy gradient methods can be applied without function approximation. However, in this discussion, we will focus on the function approximation setting, where the policy is expressed as a function of the state $s$ and a vector of parameters $\vect{\theta}$, expressed as $\pi(a|s; \vect{\theta})$.\\\\
We will begin by covering the basics of function approximation, specifically neural networks, which will serve as the foundation for all the algorithms discussed in this chapter. We will then delve into the policy gradient theorem for discrete action spaces and introduce REINFORCE, a Monte Carlo policy gradient algorithm.\\
Our main focus will be on actor-critic methods, which have achieved significant success in various domains, such as playing games like Dota 2 \cite{open-ai-five}, StarCraft II \cite{alphastar}, Go \cite{alphago-fan}, Minecraft \cite{minecraft-vpt, dreamer-v3} with also applications to language like ChatGPT \cite{instruct-gpt}. We will not provide an exhaustive overview of actor-critic methods, but we will discuss key ones, such as TRPO and PPO. The latter will also be used to in the subsequent chapter to learn to play Briscola.

\section{Function Approximation}
The methods discussed so far represent the action-value function $q(s, a)$ as a table and are thus called tabular methods. These methods have been shown to converge to an optimal policy under certain conditions, such as proper selection of the step-size $\alpha$ and the policy $\epsilon$ parameter. However, tabular methods can be impractical to use in certain scenarios, such as when the state space is extremely large (e.g. in the game of chess) or when the state space is continuous. In these cases, function approximation must be used to represent the value function, action-value function, and policy.\\\\
To keep things clear, we will focus on approximating the value function, denoted as $\hat{v}(s; \vect{w})$, where $\vect{w}$ is a parameter vector. The same ideas can also be applied to the action-value function or the policy. Furthermore, we assume to perform function approximation with neural networks, as they are most commonly used in practice and are differentiable function approximation methods, which is required by the policy gradient theorem.\\\\
Consider the problem of finding the value function corresponding to a policy $\pi$, thus far we implemented the update rule for the value function by moving the value prediction $V(s)$ closer to the return $G_t$, which represents the target:
\begin{equation}
    V(s) = V(s) + \alpha (G_t - V(s))
    \label{eq:tabular-update}
\end{equation}
With function approximation the idea is the same, moving the prediction closer to the target. But instead of updating the value of the table entry $V(s)$ directly, we need to update the parameter vector $\vect{w}$. The way we usually update the parameter vector $\vect{w}$ in neural networks is with gradient descent, that is, we update $\vect{w}$ in the negative direction of the gradient of a loss function $L$.
\begin{equation}
    \vect{w} = \vect{w} - \alpha \nabla_w L(\hat{v}(s; \vect{w}), G_t)
    \label{eq:nn-update}
\end{equation}
The loss function measures how far the value prediction $\hat{v}(s; \vect{w})$ is from the target $G_t$. A natural loss function for learning the value is the \textit{mean squared error loss} denoted as $L_{MSE}$:
\begin{equation}
    L_{MSE} = \frac{1}{2}(G_t - \hat{v}(S_t; \vect{w}))^2
    \label{eq:mse}
\end{equation}
The gradient of the loss function \eqref{eq:mse} with respect to $\vect{w}$, can be quickly computed:
\begin{equation}
    \nabla_{w} \mathrm{MSE} = \left( \hat{v}(S_t; \vect{w}) - G_t \right) \nabla_w \hat{v}(S_t; \vect{w})
    \label{eq:mse-gradient}
\end{equation}
Where $\nabla_w \hat{v}(s; \vect{w})$ is the gradient of the output of the function with respect to the parameter vector $\vect{w}$ and can be quickly found with automatic differentiation techniques.\\\\
We can view tabular methods as special case of function approximation where we have one parameter for each state, representing the value of that state. In this setting, the gradient of the loss function is simply the one-hot vector corresponding to state $S_t$:
\begin{equation}
    \nabla_w \mathrm{MSE} = \left( \hat{v}(S_t; \vect{w}) - G_t \right) \delta_{S_t}
    \label{eq:tabular-mse-gradient}
\end{equation}
Where the one-hot vector $\delta_{S_t}$ is a vector of zeros with a one in the position corresponding to the state $S_t$. The update rule \eqref{eq:nn-update} then becomes:
\begin{equation}
    \vect{w} = \vect{w} - \alpha \left( \hat{v}(S_t; \vect{w}) - G_t \right) \delta_{S_t}
    \label{eq:approximation-tabular-update}
\end{equation}
Which is exactly the update rule \eqref{eq:tabular-update} for tabular methods. Function approximation is hence a generalization of tabular methods, where the parameter vector $\vect{w}$ replaces the table of values.\\\\
Note that in the gradient derivation \eqref{eq:mse-gradient} we assumed $\nabla_w G_t = 0$, while this is true for Monte Carlo methods, where $G_t = R_{t+1} + \gamma R_{t+2} + \dots + R_T$, it is not in general true for TD methods, where the one-step return is used $G_t = R_{t+1} + \gamma \hat{v}(S_{t+1}; \vect{w})$. In this case, the gradient is not a true-gradient but a semi-gradient, because we left out the $\nabla_w G_t = \gamma \nabla_w \hat{v}(S_{t+1}; \vect{w})$ term.\\
It can be shown that semi-gradient methods have better convergence properties than true-gradient ones, in particular true-gradient methods even in the tabular case won't converge to the true value function $v_{\pi}$ in stochastic environments. The interested reader can find an example of this behavior in chapter 11.5 of the Sutton and Barto book \cite{sutton-barto}.

\subsection{Policy Approximation}
In policy gradient methods, the policy can be parameterized in any way, as long as it is differentiable with respect to the parameter vector $\vect{\theta}$. Approximating the policy directly has many advantages over approximating the action-value function:
\begin{itemize}
    \item The approximate policy can approach a deterministic policy, whereas with $\epsilon$-greedy action selection the policy always has a probability of selecting a random action.
    \item It enables the selection of actions with arbitrary probabilities, for example, in imperfect information games performing always the same action is not optimal, because the opponent can learn the player's strategy and exploit it. Think about the game of poker, the best strategy isn't to always bluff or always check, but to do a mix of both, so that the opponent can't predict the player's behavior.
    \item The action probabilities change smoothly with the parameter vector $\vect{\theta}$, whereas with $\epsilon$-greedy action selection the probability can change dramatically for a small change in the action-value function.
    \item Because of the previous point, stronger convergence guarantees are available for policy-gradient methods.
\end{itemize}

\section{Policy Gradient Theorem}
To understand the policy gradient theorem, we need to define a performance measure of a policy $\pi$ with respect to a parameter vector $\vect{\theta}$:
\begin{equation}
    J(\vect{\theta}) = v_{\pi_{\theta}}(s_0) = \sum_a \pi_{\theta}(a|s_0) q_{\pi_{\theta}}(s_0, a)
    \label{eq:performance-measure}
\end{equation}
Where $v_{\pi_{\theta}}$ represents the true value function for the policy $\pi_{\theta}$, determined by the parameter vector $\vect{\theta}$.\\\\
The problem with equation \eqref{eq:performance-measure} is that its gradient depends also on the gradient of the action-value function $q_{\pi_{\theta}}$, which in turn depends on the state distribution $\mu(s)$ which is unknown. Fortunately $\nabla_{\theta} J(\theta)$ can still be computed via the policy gradient theorem, proven in chapter 13.2 of the Sutton and Barto book \cite{sutton-barto}:
\begin{equation}
    \begin{split}
        \nabla_\theta J(\vect{\theta}) & \propto \sum_s \mu(s) \sum_a q_{\pi}(s, a) \nabla_{\theta}\pi_{\theta}(a|S_t)\\
        & = \mathbb{E}_{\pi_{\theta}} \bigg[\sum_a q_{\pi}(S_t, a) \nabla_{\theta}\pi_{\theta}(a|S_t) \bigg]
    \end{split}
    \label{eq:policy-gradient-theorem}
\end{equation}
The proof relies on approximating the state distribution $\mu(s)$ with the empirical distribution of states visited during policy evaluation. This enables us to estimate the gradient of the performance measure \eqref{eq:performance-measure} with respect to the policy parameters without needing the derivative of the state distribution.\\\\
The policy gradient theorem is a fundamental result in reinforcement learning that gives a way to compute the gradient of the performance measure with respect to the policy parameters.\\
We can further simplify expression \eqref{eq:policy-gradient-theorem} by multiplying and dividing by $\pi_{\theta}(a|S_t)$, removing the internal sum over actions:
\begin{equation}
    \begin{split}
        \nabla_\theta J(\vect{\theta}) & \propto \mathbb{E}_{\pi_{\theta}} \bigg[\sum_a q_{\pi}(S_t, a) \nabla_{\theta}\pi_{\theta}(a|S_t) \bigg]\\
        & = \mathbb{E}_{\pi_{\theta}} \bigg[q_{\pi}(S_t, A_t) \frac{\nabla_{\theta} \pi_{\theta}(A_t|S_t)}{\pi_{\theta}(A_t|S_t)} \bigg]\\
        & = \mathbb{E}_{\pi_{\theta}} \bigg[q_{\pi}(S_t, A_t) \nabla_{\theta} \log \pi_{\theta}(A_t|S_t) \bigg]
    \end{split}
    \label{eq:single-action-policy-gradient}
\end{equation}
Where in the last step we used the equality $\nabla_{\theta} \pi_{\theta}(a|s) / \pi_{\theta}(a|s) = \nabla_{\theta} \log \pi_{\theta}(a|s)$, which is true for any differentiable function $\pi_{\theta}(a|s)$.\\ This is the most commonly used form of the policy gradient theorem and allows the computation of the policy gradient even with continuous action spaces.

\section{REINFORCE}
REINFORCE estimates the action-value function $q_{\pi_{\theta}}(S_t, A_t)$ of the policy gradient (\ref{eq:single-action-policy-gradient}) using Monte Carlo.
\begin{equation}
    q_{\pi}(S_t, A_t) = \sum_{t'=t}^T \gamma^{t'-t} R_{t' + 1} = G_t
\end{equation}
Thus the update rule for the policy parameter vector $\vect{\theta}$ is:
\begin{equation}
    \theta_{t+1} = \theta_t + \alpha G_t \frac{\nabla_{\theta} \pi_{\theta}(A_t|S_t)}{\pi_{\theta}(A_t|S_t)}
    \label{eq:reinforce-update}
\end{equation}
This update increases the probability of actions proportional to the return and inversely proportional to the probability of the action. This makes sense because we move towards actions that lead to a higher return and perform a bigger update for actions with a lower probability, which are selected and hence updated less often.

\subsection{REINFORCE with Baseline}
The policy gradient theorem \eqref{eq:policy-gradient-theorem} can be generalized by adding a baseline function $b(s)$ to the action-value function without loss of generality:
\begin{equation}
    \begin{split}
        & \sum_s \mu(s) \sum_a (q_{\pi}(s, a) - b(s)) \nabla_{\theta}\pi_{\theta}(a|S_t) = \\
        & = \sum_s \mu(s) \sum_a (q_{\pi}(s, a)) \nabla_{\theta}\pi_{\theta}(a|S_t) - \sum_s \mu(s) b(s) \sum_a \nabla_{\theta}\pi_{\theta}(a|S_t)\\
        & = \nabla_{\theta} J(\vect{\theta}) - \sum_s \mu(s) b(s) \nabla_{\theta} \sum_a \pi_{\theta}(a|s)\\
        & = \nabla_{\theta} J(\vect{\theta})
    \end{split}
    \label{eq:policy-gradient-theorem-generalized}
\end{equation}
Where in the last step we used the fact that $\nabla_{\theta} \sum_a \pi_{\theta}(a|s) = 0$. The baseline function $b(s)$ can be any function of the state $s$ and is used to reduce the variance of the gradient estimator. However, one natural choice for the baseline function is the value function $v_{\pi_{\theta}}(s)$, which gives the REINFORCE with baseline update:
\begin{equation}
    \theta_{t+1} = \theta_t + \alpha (G_t - \hat{v}_{\pi_{\theta}}(S_t)) \frac{\nabla_{\theta} \pi_{\theta}(A_t|S_t)}{\pi_{\theta}(A_t|S_t)}
    \label{eq:reinforce-with-baseline-update}
\end{equation}
Where $\hat{v}_{\pi_{\theta}}(S_t)$ is estimated with Monte Carlo. The improvement of the REINFORCE with baseline algorithm over the REINFORCE algorithm is shown in Figure \ref{fig:reinforce-comparison}.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/reinforce-comparison.png}
    \caption{Comparison of the REINFORCE and REINFORCE with baseline algorithms on the short-corridor gridworld, with the optimal step sizes indicated. The results show that the baseline algorithm outperforms the REINFORCE algorithm, reaching the best return in just 100 steps, while the REINFORCE algorithm takes roughly 1000 steps.}
    \label{fig:reinforce-comparison}
\end{figure}

\section{Actor-Critic Methods}
Actor-Critic methods merge policy gradient and temporal difference methods. The policy gradient is used to update the policy parameter vector $\vect{\theta}$ and TD-learning is used to update the value function $v_{\pi_{\theta}}(s)$. This introduces a bias in the policy gradient update, however it also reduces variance and hence accelerates learning.\\
One-step actor-critic methods updates the policy as following:
\begin{equation}
    \theta_{t+1} = \theta_t + \alpha (R_{t+1} + \gamma \hat{v}_{\pi_{\theta}}(S_{t+1}; \vect{w}) - \hat{v}_{\pi_{\theta}}(S_t; \vect{w})) \frac{\nabla_{\theta} \pi_{\theta}(A_t|S_t)}{\pi_{\theta}(A_t|S_t)}
    \label{eq:one-step-actor-critic-update}
\end{equation}
Which is similar to the REINFORCE with baseline update \eqref{eq:reinforce-with-baseline-update}, but uses the one-step TD target $G_{t:t+1} = R_{t+1} + \gamma \hat{v}(S_{t+1}; \vect{w})$ instead of the full return $G_t$.\\
At the same time the value function is updated using the TD error:
\begin{equation}
    \vect{w}_{t+1} = \vect{w}_t + \alpha (R_{t+1} + \gamma \hat{v}_{\pi_{\theta}}(S_{t+1}; \vect{w}) - \hat{v}_{\pi_{\theta}}(S_t; \vect{w})) \nabla_{\vect{w}} \hat{v}_{\pi_{\theta}}(S_t; \vect{w})
    \label{eq:one-step-actor-critic-value-function-update}
\end{equation}
These methods are called actor-critic methods because the policy is called the actor and the value function is called the critic. The critic learns the value function and the actor, given the value function, learns the policy.

\section{Trust Region Policy Optimization}