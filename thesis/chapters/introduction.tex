\chapter{Introduction}
Briscola is one of Italy's most popular card games \cite{briscola-wikipedia} and variations of it are also played in Portugal, Slovenia, Croatia and Montenegro. It can be played from two to six players. Briscola presents unique challenges for reinforcement learning due to its hidden information aspect. Unlike perfect information games like checkers, chess, and Go, established techniques cannot be easily applied to Briscola. Reinforcement learning in Briscola requires finding the best move under incomplete information about the game state, forcing the agent to make educated guesses about the cards held by other players.\\\\
In this thesis, we propose a new algorithm to learning to play Briscola in a one-on-one setting based on policy gradient methods with function approximation. Specifically, we adopt PPO with self-play, following the approach of OpenAI on Dota 2 \cite{open-ai-five}.\\
The policy gradient method is chosen due to its ability to learn any strategy, rather than being restricted to a deterministic one. This is crucial in learning to play Briscola, an imperfect information game, as a deterministic strategy will not necessarily result in the optimal play. To better understand this point, consider the game of poker. In a given situation, the best move is not always to check or to fold, but rather a combination of the two, in order to prevent the opponent from being able to determine what cards the agent is holding.\\\\
Our approach was able to learn how to play Briscola in a one-on-one setting in less than 20 minutes on a single computer, making it computationally efficient and easily replicable.\\
A web-based version of the game where the reader can play against the trained agent is available at \url{https://replit.com/@LorenzoCavuoti/BriscolaBot}.

\section{Game Theory}
Game theory is a branch of mathematics that focuses on the study of strategies for handling situations where the result of a player's action depends on the choices made by other participants.\\\\
Games like chess and Go are referred to as perfect information, zero-sum games. Perfect information means that all players have access to all information about the game state, while zero-sum means that what benefits one player harms the other, with no possibility for a win-win outcome. There are various techniques for finding or approximating the optimal strategy\footnote{The optimal strategy is the strategy that gives us the best win chance, assuming that the opponent plays perfectly.} in such games, like minimax search and Monte Carlo tree search (MCTS).

For instance, the Stockfish chess engine utilizes a form of minimax search called alpha-beta pruning, along with a heuristic evaluation function, to determine the best move \cite{stockfish}. Meanwhile, the Leela Chess Zero chess engine uses MCTS guided by a neural network \cite{lc0}.\\\\
In contrast, games like Briscola, poker, StarCraft II, Dota 2, and Kriegspiel are imperfect information, zero-sum games, imperfect information means that the game state is not fully observable. As a result, agents must make assumptions about the game state. In these games, the optimal play strategy is not deterministic, as a deterministic agent can be predictable and exploited by the opponent. When choosing the best move the agent must also take into account the information gained and the information that the agent might give to the opponent \cite{norvig-russell}.

Imperfect information games can still be solved by finding or approximating a Nash equilibrium. In a Nash equilibrium, no player has an incentive to change their strategy, as this will lead to a worse outcome for them. John Nash demonstrated in 1952 that every game with a finite number of players and actions has at least one Nash equilibrium \cite{nash-equilibrium-wikipedia}.


\section{Related work}
Existing approaches to learning Briscola can be broadly classified into three categories:
\begin{enumerate}
    \item \textbf{Rule based agents:} These agents use pre-determined, human-created rules to play the game. However, they lack the ability to adapt to new situations, as they are unable to learn from experience. The website \url{http://www.solitariconlecarte.it/briscola_2classica.htm} provides a rule based agent that plays Briscola.
    \item \textbf{Model-free agents:} These agents use model-free reinforcement learning techniques to learn the optimal strategy. They learn from experience through self-play or playing against other agents, such as a rule-based agent. At the time of writing, there are two known projects that use Deep Q-Learning (DQL) \cite{mnih2013playing} to learn Briscola. The first one \cite{fezriva-briscola-dqn} trains the DQL agent against a random agent, while the second \cite{alsora-deep-briscola-dqn} uses self-play to reach 90\% win rate against a random agent.
    \item \textbf{Search-based agents:} These agents employ Monte Carlo tree search (MCTS) to explore the game tree and find the best move. This approach has been successful in perfect information games like chess \cite{alphazero}, Go \cite{alphagozero} and backgammon \cite{td-gammon}, but requires a model of the environment to be effective. In the case of imperfect information games like Briscola, this model is not available as the strategies of other players are unknown. To overcome this, some approaches assume the other player will randomly play a card from the set of unseen cards \cite{Briscola-mcts-Playing-Algorithm}. A more flexible approach could involve learning a model of the other player to predict their moves, allowing the agent to adapt to their strategy.
\end{enumerate}
These approaches have also been combined, although not in Briscola. For instance, one of the earliest versions of AlphaGo \cite{alphago-fan} first learned Go strategies from professional human players, then refined the learned strategy using model-free reinforcement learning techniques. Lastly, it utilized Monte Carlo Tree Search (MCTS) to further enhance its performance during actual gameplay.\\\\
While developing our algorithm, we also looked at other card games such as poker, which has received significant attention over the years due to its popularity. In 2017, Libratus \cite{libratus} became the first program to defeat the world's best poker players by using the Counterfactual Regret Minimization (CFR) \cite{cfr} algorithm. CFR is an iterative method that converges to a Nash equilibrium in two-player zero-sum games like poker or Briscola. Recently, in 2020, ReBeL \cite{rebel} became the first program to beat the world's best poker players in full-table, not just one-on-one, no-limit Texas hold'em. ReBeL used a similar approach to CFR, but with the added use of deep neural networks to guide the algorithm, reducing the computational time required to reach the Nash equilibrium.

\section{Contributions}
The key contributions of this thesis are:
\begin{itemize}
    \item Presenting a novel algorithm for learning to play Briscola in a one-on-one setting using policy gradient methods with function approximation, based on PPO with self-play.
    \item Demonstrating that the algorithm achieves human-level play in less than 20 minutes of training on a standard 4-core computer, highlighting both its effectiveness and computational efficiency.
    \item Conducting ablation studies on the reward structure (comparing rewards based on points scored vs game outcome), evaluating the effectiveness of self-play, and exploring the impact of other hyperparameters.
    \item Developing a web-based version of the game where users can play Briscola against the trained agent, available at \url{https://replit.com/@LorenzoCavuoti/BriscolaBot}.
\end{itemize}

\section{Outline}
The rest of the thesis is organized as follows:
DA SCRIVERE