\chapter{Introduction}
Briscola is one of Italy's most popular card games \cite{briscola-wikipedia} and variations of it are also played in Portugal, Slovenia, Croatia and Montenegro. It can be played from two to six players. Briscola presents unique challenges for reinforcement learning due to its hidden information aspect. Unlike perfect information games like checkers, chess, and Go, established techniques cannot be easily applied to Briscola. Reinforcement learning in Briscola requires finding the best move under incomplete information about the game state, forcing the agent to make educated guesses about the cards held by other players.\\
In this thesis, we propose a new algorithm to learning to play Briscola in a one-on-one setting based on policy gradient methods with function approximation, in particular we used PPO with self-play following the approach of OpenAI on Dota 2 \cite{open-ai-five}.\\
We used a policy-gradient method because it can learn any strategy, without being constrained to a deterministic one. This property is essential when learning to play an imperfect information game like Briscola, because a deterministic strategy won't lead to the best strategy. To make this statement intuitive think of poker, the best move in a given situation is neither to check nor to fold, but to sometimes check, and other times fold, so that the opponent cannot guess what the agent is holding.\\\\
% We used function approximation with deep neural networks because the size of the state space of Briscola is exponential in the number of cards in the deck, thus making impossible to use a table-based approach to store the agent policy and value function.\\\\
This approach manages to learn to play Briscola in a one-on-one setting in less than 20 minutes on a single computer, making the approach computationally efficient and easy to replicate.\\\\
A web-based version of the game where the reader can play Briscola against the trained agent can be found at \href{https://replit.com/@LorenzoCavuoti/BriscolaBot}{https://replit.com/@LorenzoCavuoti/BriscolaBot}.\\\\

\section{Related work}
Current approaches to learning Briscola follow three main directions:
\begin{enumerate}
    \item \textbf{Rule based agents} use a set of human-coded rules to play the game. These agents are not able to learn from experience thus are not able to adapt to new situations. The website \url{http://www.solitariconlecarte.it/briscola_2classica.htm} provides a rule based agent that plays Briscola.
    \item \textbf{Model-free agents} use model-free reinforcement learning methods to learn the optimal strategy. These agents learn from experience by either playing against themselves or against another agent, like a hand-coded rule based agent. At the time of writing we have found two projects that use model-free reinforcement learning to learn Briscola \cite{fezriva-briscola-dqn, alsora-deep-briscola-dqn}, both these projects use Deep Q-Learning \cite{mnih2013playing} to learn the optimal strategy by playing games against a random player\footnote[1]{A random player is a player that throws a random card from the set of cards it's currently holding at each turn.}.
    \item \textbf{Search-based agents} use Monte Carlo tree search to find the best move. These agents are able to find the best move by exploring the game tree, and have been applied successfully to perfect information games like chess \cite{alphazero}, Go \cite{alphagozero}, backgammon \cite{td-gammon} and many others. However, these methods need a model of the environment to be applied, while in the games stated above this model is available, in an imperfect information game like Briscola this model is not available as the other players strategies are unknown. A common way to solve this problem is to assume the other player strategies, for example \cite{Briscola-mcts-Playing-Algorithm} assumes that the opponent always plays a random card from the set of unseen cards. A more flexible approach could be to learn a model of the other player, or said in other words, predict the other player's moves, this way the agent could adapt to the opponent's strategy.
\end{enumerate}
When developing our algorithm we also looked at algorithms applied to other card games, like poker, which is particularly interesting because it has been studied a lot over the years due to its popularity. In poker in 2017 Libratus \cite{libratus} became the first program to beat the world' best poker players. Libratus used an algorithm called Counterfactual Regret Minimization (CFR) \cite{cfr} to learn the optimal strategy. CFR is able to learn the optimal strategy of any imperfect information game by approximating a Nash equilibrium. More recently, in 2020, ReBeL \cite{rebel} became the first program to beat the world's best poker players in heads-up no-limit Texas hold'em. ReBeL uses a similar approach to CFR, approximating a Nash equilibrium, but uses deep neural networks to guide the CFR algorithm, reducing the computational time required to reach the Nash equilibrium.\\\\

\section{Contributions}
The Contributions of this thesis are:
\begin{itemize}
    \item We propose a new algorithm to learn to play Briscola in a one-on-one setting based on policy gradient methods with function approximation, in particular we used PPO with self-play following the approach of OpenAI on Dota 2 \cite{open-ai-five}.
    \item asd
\end{itemize}


We show that our algorithm is able to reach average-level human performance in less than 20 minutes when trained on a single computer, making the approach computationally efficient and easy to replicate. Furthermore, we provide ablations studies on reward structure (is it better to give reward based on the points scored or whether the agent won or lost the game?), the effectiveness of self-play and other hyperparameters. Finally, we provide a web-based version of the game where the reader can play Briscola against the trained agent, at \href{https://replit.com/@LorenzoCavuoti/BriscolaBot}{https://replit.com/@LorenzoCavuoti/BriscolaBot}.\\\\

\section{Outline}