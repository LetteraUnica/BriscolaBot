\chapter{Introduction}
Briscola is one of Italy's most popular card games and variations of it are also played in Portugal, Slovenia, Croatia and Montenegro \cite{briscola-wikipedia}. It can be played from two to six players. Briscola presents unique challenges for reinforcement learning due to its hidden information aspect. Unlike perfect information games like checkers, chess, and Go, established techniques cannot be easily applied to Briscola. Reinforcement learning in Briscola requires finding the best move under incomplete information about the game state, forcing the agent to make educated guesses about the cards held by other players.\\\\
In this thesis, we propose a new algorithm to learning to play Briscola in a one-on-one setting based on policy gradient methods with function approximation. Specifically, we adopt PPO with self-play, following the approach of OpenAI on Dota 2 \cite{open-ai-five}.\\
We have chosen a policy gradient method due to its ability to learn any strategy, rather than being restricted to a deterministic one. This is crucial in learning to play Briscola, an imperfect information game, as a deterministic strategy will not necessarily result in the optimal play. To better understand this point, consider the game of poker. In a given situation, the best move is not always to check or to fold, but rather mix between the two, in order to prevent the opponent from being able to determine what cards the agent is holding. In other words, the value
of an action depends on the probability it is played.\\\\
Our approach was able to learn how to play Briscola in a one-on-one setting in using only a standard 4-core CPU, making it computationally efficient and easily replicable.\\
A web-based version of the game where the reader can play against the trained agent is available at \url{https://replit.com/@LorenzoCavuoti/BriscolaBot}.

\section{Game Theory}
Game theory is a branch of mathematics that focuses on the study of strategies for handling situations where the result of a player's action depends on the choices made by other participants.\\\\
Games like chess and Go are referred to as perfect information, zero-sum games. Perfect information means that all players have access to all information about the game state, while zero-sum means that what benefits one player harms the other, with no possibility for a win-win outcome. There are various techniques for finding or approximating the optimal strategy\footnote{The optimal strategy is the strategy that gives the best win chance, assuming that the opponent plays perfectly.} in such games, like minimax search and Monte Carlo tree search (MCTS).

For instance, the Stockfish chess engine, to determine the best move, utilizes a variant of minimax search called alpha-beta pruning with a heuristic evaluation function \cite{stockfish}. Meanwhile, the Leela Chess Zero chess engine uses MCTS guided by a neural network \cite{lc0}.\\\\
In contrast, games like Briscola, poker, StarCraft II, Dota 2, and Kriegspiel are imperfect information, zero-sum games. Imperfect information means that the game state is not fully observable, forcing agents to make inferences based on incomplete data. As a result, the optimal play strategy is not always deterministic, as a deterministic agent can be predictable and exploited by the opponent. When choosing the best move, the agent must also take into account the information gained and the information they might reveal to the opponent \cite{norvig-russell}.

Imperfect information games can still be solved by finding or approximating a Nash equilibrium. In a Nash equilibrium, no player has an incentive to change their strategy, as this will lead to a worse outcome for them. John Nash demonstrated in 1952 that every game with a finite number of players and actions has at least one Nash equilibrium \cite{nash-equilibrium-wikipedia}.


\section{Related work}
Existing approaches to learning Briscola can be broadly classified into three categories:
\begin{enumerate}
    \item \textbf{Rule based agents:} These agents use pre-determined, human-created rules to play the game. However, they lack the ability to adapt to new situations, as they are unable to learn from experience. The website \url{http://www.solitariconlecarte.it/briscola_2classica.htm} provides a rule based agent that plays Briscola.
    \item \textbf{Model-free agents:} These agents use model-free reinforcement learning techniques to learn the optimal strategy. They learn from experience through self-play or playing against other agents, such as a rule-based agent. At the time of writing, there are two known projects that use Deep Q-Learning (DQL) \cite{mnih2013playing} to learn Briscola. The first one \cite{fezriva-briscola-dqn} trains the DQL agent against a random agent, while the second \cite{alsora-deep-briscola-dqn} uses self-play to reach 90\% win rate against a random agent.
    \item \textbf{Search-based agents:} These agents employ Monte Carlo tree search (MCTS) to explore the game tree and find the best move. This approach has been successful in perfect information games like chess \cite{alphazero}, Go \cite{alphagozero} and backgammon \cite{td-gammon}, as it requires a model of the environment. In the case of imperfect information games like Briscola, this model is not available as the strategies of other players are unknown. To overcome this, some approaches assume the other player will randomly play a card from the set of unseen cards \cite{Briscola-mcts-Playing-Algorithm}. A more flexible method could involve learning a model of the other player to predicts their moves, allowing the agent to adapt to their strategy.
\end{enumerate}
These approaches have also been combined, although not in Briscola. For instance, one of the earliest versions of AlphaGo \cite{alphago-fan} learned a Go strategy by imitating professional human players, then refined the learned strategy using model-free reinforcement learning techniques. Lastly, it utilized MCTS to further enhance its performance during actual gameplay.\\\\
While developing our algorithm, we also looked at other card games such as poker, which has received significant attention over the years due to its popularity. In 2017, Libratus \cite{libratus} became the first program to defeat the world's best poker players by using the Counterfactual Regret Minimization (CFR) \cite{cfr} algorithm. CFR is an iterative method that converges to a Nash equilibrium in two-player zero-sum games like poker or Briscola. Recently, in 2020, ReBeL \cite{rebel} became the first program to beat the world's best poker players in full-table, not just one-on-one, no-limit Texas hold'em. ReBeL used a similar approach to CFR, but with the addition of a deep neural network to guide the algorithm, reducing the computational time required to reach the Nash equilibrium.

\section{Contributions}
The key contributions of this thesis are:
\begin{itemize}
    \item Presenting a novel algorithm for learning to play Briscola in a one-on-one setting using policy gradient methods with function approximation, based on PPO with self-play.
    \item Demonstrating that the algorithm achieves human-level play when trained on a standard 4-core computer, highlighting both its effectiveness and computational efficiency.
    \item Exploring the impact of the reward structure on the agent's performance and examining the influence of additional hyperparameters.
    \item Developing a web-based version of the game where users can play Briscola against the trained agent, available at \url{https://replit.com/@LorenzoCavuoti/BriscolaBot}.
\end{itemize}

\section{Outline}
The rest of the thesis is organized as follows:\\
\textbf{Chapter 1} provides a theoretical introduction to Reinforcement Learning (RL), its relationship to Markov Decision Processes (MDPs), and the fundamental concepts and elements involved. It defines rewards, policies, and value functions, and delves into the algorithmic techniques used to solve RL problems such as Dynamic Programming, Monte Carlo methods, Temporal-Difference Learning, and $n$-step methods. The chapter concludes by exploring the challenges posed by Multi-Agent Reinforcement Learning. The chapter concludes by introducing the problem of Multi-Agent Reinforcement Learning and explores its challenges.
\textbf{Chapter 3} describes policy gradient methods with function approximation. The chapter begins with an overview of function approximation techniques, in particular neural networks, followed by the presentation of the policy gradient theorem - a key result in reinforcement learning. Next, policy gradient algorithms such as REINFORCE and its baseline version are discussed in detail. Lastly, the chapter delves into actor-critic methods such as TRPO and PPO, with a focus on the latter which has been utilized in major publications, ranging from natural language \cite{instruct-gpt} to the game of Dota 2 \cite{open-ai-five}.
\textbf{Chapter 4} presents the core contribution of this thesis. It details the design and implementation of the agent and the environment in which it operates, like the definition of the reward structure and observation space. The chapter also describes thoroughly the training procedure that we followed, including the choice of hyperparameters and their impact on the agent's performance. At the end of the chapter, we evaluate the last version of the agent, BriscolaBot-v3, against human players, showcasing the effectiveness of the proposed solution to the problem.
\textbf{Chapter 5} summarizes the results of the presented approach, highlighting the major difficulties posed by the problem, and offers suggestions for future improvements. 