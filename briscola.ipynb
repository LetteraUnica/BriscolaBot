{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim, Tensor\n",
    "\n",
    "from src.agents import Agent\n",
    "from src.agents.NNAgent import NNAgent\n",
    "from src.agents.RandomAgent import RandomAgent\n",
    "from src.envs.two_player_briscola.TwoPlayerBriscola import TwoPlayerBriscola as Briscola\n",
    "from time import time\n",
    "import gymnasium as gym\n",
    "import wandb\n",
    "\n",
    "from src.vectorizers.VectorizedEnv import VectorizedEnv\n",
    "from src.envs.two_player_briscola.TwoPlayerBriscola import TwoPlayerBriscola\n",
    "\n",
    "from src.envs.two_player_briscola.BriscolaConstants import Constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "## TODO: Test briscola\n",
    "env = Briscola()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_envs\" : 1024,\n",
    "    \"n_steps\": Constants.deck_cards // 2,\n",
    "    \"lr\": 2.5e-4,\n",
    "    \"mini_batch_size\": 256,\n",
    "    \"total_timesteps\": 10000,\n",
    "    \"gamma\": 1.,\n",
    "    \"lambda\": 0.95\n",
    "}\n",
    "params[\"batch_size\"] = params[\"n_envs\"] * params[\"n_steps\"]\n",
    "params[\"num_updates\"] = params[\"total_timesteps\"] // params[\"mini_batch_size\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_4592\\433482101.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mproject\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"briscolaBot\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mentity\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"lettera\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m     \u001B[0msave_code\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m )\n",
      "\u001B[1;31mNameError\u001B[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"briscolaBot\",\n",
    "    entity=\"lettera\",\n",
    "    config=params,\n",
    "    save_code=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def play_other_player_moves(envs: VectorizedEnv, policy: Agent):\n",
    "    for _ in range(2):\n",
    "        envs_to_play = [env for env in envs.envs\n",
    "                        if env.agent_selection == env.agents[1]\n",
    "                        and not env.terminations[env.agents[1]]]\n",
    "\n",
    "        obs, action_mask = [], []\n",
    "        for env in envs_to_play:\n",
    "            observation = env.observe(env.agent_selection)\n",
    "            obs.append(observation[\"observation\"])\n",
    "            action_mask.append(observation[\"action_mask\"])\n",
    "\n",
    "        obs, action_mask = np.array(obs), np.array(action_mask)\n",
    "        actions = policy.get_action(torch.tensor(obs).to(device), torch.tensor(action_mask).to(device))\n",
    "\n",
    "        [env.step(action) for env, action in zip(envs_to_play, actions)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_state_representation(envs: VectorizedEnv) -> tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    obs = np.empty((len(envs),) + envs.single_observation_space()[\"observation\"].shape, dtype=np.float32)\n",
    "    action_masks = np.empty((len(envs), envs.single_action_space().n), dtype=np.int8)\n",
    "    rewards = np.empty(len(envs), dtype=np.float32)\n",
    "    dones = np.empty(len(envs), dtype=np.int8)\n",
    "    for i, (observation, reward, termination, _, _) in enumerate(envs.last()):\n",
    "        obs[i] = observation[\"observation\"]\n",
    "        action_masks[i] = observation[\"action_mask\"]\n",
    "        rewards[i] = reward\n",
    "        dones[i] = termination\n",
    "\n",
    "    return torch.tensor(obs), torch.tensor(action_masks), torch.tensor(rewards), torch.tensor(dones)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "envs = VectorizedEnv(lambda : TwoPlayerBriscola(), params[\"n_envs\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "observation_shape = envs.single_observation_space()[\"observation\"].shape\n",
    "action_size = envs.single_action_space().n\n",
    "\n",
    "agent = NNAgent(observation_shape, action_size).to(device)\n",
    "\n",
    "other_player = RandomAgent(Constants.hand_cards)\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr=params[\"lr\"], eps=1e-5)\n",
    "\n",
    "obs = torch.zeros((params[\"n_steps\"], params[\"n_envs\"]) + observation_shape).to(device)\n",
    "actions = torch.zeros((params[\"n_steps\"], params[\"n_envs\"]), dtype=torch.int8).to(device)\n",
    "logprobs = torch.zeros((params[\"n_steps\"], params[\"n_envs\"])).to(device)\n",
    "rewards = torch.zeros((params[\"n_steps\"], params[\"n_envs\"])).to(device)\n",
    "dones = torch.zeros((params[\"n_steps\"], params[\"n_envs\"]), dtype=torch.int8).to(device)\n",
    "values = torch.zeros((params[\"n_steps\"], params[\"n_envs\"])).to(device)\n",
    "\n",
    "global_step = 0\n",
    "play_other_player_moves(envs, other_player)\n",
    "next_obs, action_mask, reward, next_done = get_state_representation(envs)\n",
    "\n",
    "for update in range(1, params[\"num_updates\"] + 1):\n",
    "    # Play episodes\n",
    "    for step in range(params[\"n_steps\"]):\n",
    "        global_step += params[\"n_envs\"]\n",
    "\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs.to(device), action_mask.to(device))\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        envs.step(actions[step].cpu().numpy())\n",
    "        play_other_player_moves(envs, other_player)\n",
    "        next_obs, action_mask, reward, next_done = get_state_representation(envs)\n",
    "        rewards[step] = reward.to(device)\n",
    "\n",
    "    # Bootstrap objective\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        last_gae_lambda = 0\n",
    "\n",
    "        for t in reversed(range(params[\"n_steps\"])):\n",
    "            if t == params[\"n_steps\"] - 1:\n",
    "                next_non_terminal = 1. - next_done\n",
    "                next_values = next_value\n",
    "            else:\n",
    "                next_non_terminal = 1. - dones[t+1]\n",
    "                next_values = values[t+1]\n",
    "\n",
    "            delta = rewards[t] + params[\"gamma\"] * next_values * next_non_terminal - values[t]\n",
    "            last_gae_lambda = delta + params[\"gamma\"] * params[\"lambda\"] * next_non_terminal * last_gae_lambda\n",
    "            advantages[t] = last_gae_lambda\n",
    "\n",
    "        returns = advantages + values\n",
    "\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
