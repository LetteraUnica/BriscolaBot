{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "import pylab as pl\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from torch import optim, Tensor, nn\n",
    "\n",
    "from src.agents import Agent\n",
    "from src.agents.NNAgent import NNAgent\n",
    "from src.agents.RandomAgent import RandomAgent\n",
    "from src.envs.two_player_briscola.TwoPlayerBriscola import TwoPlayerBriscola as Briscola\n",
    "from time import time\n",
    "import gymnasium as gym\n",
    "import wandb\n",
    "from src.utils.AgentPool import AgentPool\n",
    "from src.utils.training_utils import play_all_moves_of_player, get_state_representation\n",
    "\n",
    "from src.vectorizers.VectorizedEnv import VectorizedEnv\n",
    "from src.envs.two_player_briscola.TwoPlayerBriscola import TwoPlayerBriscola\n",
    "\n",
    "from src.envs.two_player_briscola.BriscolaConstants import Constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_envs\" : 2048,\n",
    "    \"n_steps\": Constants.deck_cards // 2,\n",
    "    \"lr\": 3e-3,\n",
    "    \"lr_decay\": 0.995,\n",
    "    \"lr_min\": 3e-4,\n",
    "    \"mini_batch_size\": 1024,\n",
    "    \"total_timesteps\": 20_000_000,\n",
    "    \"gamma\": 1.,\n",
    "    \"lambda\": 0.9,\n",
    "    \"update_epochs\": 2,\n",
    "    \"clip_coef\": 0.3,\n",
    "    \"normalize_advantage\": True,\n",
    "    \"clip_value_loss\": True,\n",
    "    \"value_coef\": 0.5,\n",
    "    \"entropy_coef\": 1e-2,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"ratio_win_reward\": 0.1,\n",
    "    \"n_opponents\": 8,\n",
    "    \"max_pool_size\": 128,\n",
    "    \"add_model_every_x_step\": 1,\n",
    "    \"nu\": 0.1,\n",
    "    \"self_play_opponents\": 1,\n",
    "    \"hidden_size\": 128,\n",
    "    \"n_layers\": 3\n",
    "}\n",
    "params[\"batch_size\"] = params[\"n_envs\"] * params[\"n_steps\"]\n",
    "params[\"num_updates\"] = params[\"total_timesteps\"] // params[\"batch_size\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mlettera\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.13.8 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.7"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\loren\\Desktop\\rl-thesis\\wandb\\run-20230111_001920-2pymwcoy</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/lettera/briscolaBot/runs/2pymwcoy\" target=\"_blank\">smaller lr decay</a></strong> to <a href=\"https://wandb.ai/lettera/briscolaBot\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "        name=\"smaller lr decay\",\n",
    "        project=\"briscolaBot\",\n",
    "        entity=\"lettera\",\n",
    "        config=params,\n",
    "        save_code=True,\n",
    "        sync_tensorboard=False,\n",
    "        mode=\"online\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def export_to_onnx(agent: NNAgent, filename=\"agent.onnx\", output_names=None, input_names=None):\n",
    "    if output_names is None:\n",
    "        output_names = [\"action\"]\n",
    "\n",
    "    if input_names is None:\n",
    "        input_names = [\"input\"]\n",
    "\n",
    "    dummy_input = torch.randn(1, np.prod(agent.observation_shape) + agent.action_size)\n",
    "    torch.onnx.export(agent, dummy_input, filename, verbose=False, input_names=input_names, output_names=output_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 162]) torch.Size([1, 40])\n",
      "tensor([[-1.0150, -0.4085, -0.3272,  0.6845, -0.0116,  0.0833, -0.4302, -0.5566,\n",
      "         -0.6885,  1.3901,  1.1319,  0.5917, -1.0315,  1.0796, -1.3298, -0.2176,\n",
      "         -0.9626, -2.0981, -0.8719,  0.7350,  1.3269, -1.5413,  0.2369, -0.4059,\n",
      "         -0.1600,  2.1844, -0.1539,  2.1859,  0.5690,  1.0467, -1.1876, -1.2035,\n",
      "          0.1239, -0.4494, -0.1965, -0.4781,  0.1948, -0.5073,  0.6364, -0.9575,\n",
      "         -1.8515, -2.3515, -0.7917, -3.0825, -1.0261,  0.7789, -1.7436,  1.6456,\n",
      "         -1.3087,  2.1712, -0.3974,  0.1378, -0.0355,  1.6550,  0.5184, -0.4801,\n",
      "          1.3318, -0.5009,  0.9421, -0.5971,  0.4015, -1.0337, -0.5593,  0.3144,\n",
      "         -0.8246, -1.1667,  0.0427,  0.4034, -0.4306,  0.4908,  0.9820,  0.1247,\n",
      "         -0.2118,  1.0713, -0.6272,  0.0734, -0.6540, -1.4395, -0.5973, -2.1780,\n",
      "          0.0998,  1.9429, -1.0462, -0.6673,  0.4035,  0.3358,  1.2647, -0.1966,\n",
      "          0.0652, -2.1604,  0.2765, -0.7375, -0.7879,  0.4022,  1.5027, -1.1861,\n",
      "          1.3098, -0.6154,  0.3586, -1.0349, -1.7914,  0.2283, -0.3830,  0.8459,\n",
      "         -1.4343, -0.9850, -0.9238,  0.2325, -0.2957, -0.1952, -2.9595, -0.4266,\n",
      "         -0.1608,  0.5331,  0.3907, -0.4511,  0.6878, -0.5388, -1.7453, -0.5385,\n",
      "         -0.3244, -1.6464, -0.8020,  2.6873, -0.4869, -0.3075,  1.5698, -0.0585,\n",
      "         -0.5131,  1.2380,  0.6795, -2.1122, -0.1952, -1.0410,  1.7754, -1.0891,\n",
      "         -0.9067, -1.3611,  2.4312,  0.4816, -0.2400,  0.4671, -0.0987,  0.4636,\n",
      "          0.6683,  1.0447, -0.8289,  0.9782, -0.4526,  1.3920, -1.6585,  0.4761,\n",
      "         -0.4200, -0.3180,  0.1816,  0.9514,  1.4120, -0.1669, -0.0601, -1.2409,\n",
      "         -1.7052,  0.8897]]) tensor([[ 1.1819, -0.9141,  1.1548, -0.3782,  0.3560,  1.9844, -1.8699,  0.4022,\n",
      "          1.0417,  1.3023,  1.2038, -0.7367, -0.8934,  0.2229,  0.3000,  0.6619,\n",
      "          0.6571,  0.4602, -2.1055, -0.8316, -0.5809,  0.6766,  0.3547, -0.0455,\n",
      "         -1.8379, -1.1287,  0.4656, -0.2013, -0.0980, -0.8523,  0.2923, -1.2130,\n",
      "         -2.0771, -0.8826,  1.4614,  0.5807,  0.9913,  0.1577,  0.9554,  0.7479]])\n",
      "========== Diagnostic Run torch.onnx.export version 2.0.0.dev20230105 ==========\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = NNAgent((162, ), 40, hidden_size=256)\n",
    "agent.load_state_dict(torch.load(\"agent.pt\"))\n",
    "\n",
    "export_to_onnx(agent)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([10], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(\"agent.onnx\")\n",
    "\n",
    "inputs = np.random.randn(1, 162+40).astype(np.float32)\n",
    "inputs[:, -40:] = 0\n",
    "inputs[:, -30] = 1\n",
    "\n",
    "outputs = ort_session.run(\n",
    "    [\"action\"],\n",
    "    {\"input\": inputs}\n",
    ")\n",
    "print(outputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from scipy.special import logit\n",
    "\n",
    "\n",
    "def compute_rating(current_policy: Agent,\n",
    "                   other_player_policy: Agent,\n",
    "                   n_games: int = 512,\n",
    "                   env_fn = lambda: TwoPlayerBriscola(),\n",
    "                   current_player: str = \"player_0\",\n",
    "                   other_player: str = \"player_1\"):\n",
    "    vec_env = VectorizedEnv(env_fn, n_games)\n",
    "    vec_env.reset()\n",
    "    other_player_policies = [other_player_policy]\n",
    "    for step in range(params[\"n_steps\"]):\n",
    "        with torch.no_grad():\n",
    "            play_all_moves_of_players(vec_env, other_player_policies, other_player)\n",
    "            next_obs, action_mask, _, _ = get_state_representation(vec_env)\n",
    "            actions = current_policy.get_actions(next_obs.to(device), action_mask.to(device))\n",
    "\n",
    "        vec_env.step(actions.cpu().numpy())\n",
    "    play_all_moves_of_players(vec_env, other_player_policies, \"player_1\")  # Play the last move\n",
    "\n",
    "    scores = np.array([env.get_game_outcome(current_player) for env in vec_env.get_envs()], dtype=np.float64)\n",
    "    mean_score = np.mean(scores)\n",
    "    return mean_score, logit(mean_score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "vec_env = VectorizedEnv(lambda: TwoPlayerBriscola(), params[\"n_envs\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 35 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 35 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 39 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 17 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 19 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 1 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 24 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 20 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 9 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 39 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 16 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 18 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 0 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 20 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 33 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 7 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 2 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 32 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 32 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 20 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 7 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 12 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 31 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 32 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 25 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 34 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 30 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 2 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 26 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 37 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 0 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 37 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 30 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 20 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 25 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 8 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 28 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 9 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 19 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 13 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 33 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 32 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 2 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 32 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 36 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n",
      "C:\\Users\\loren\\Desktop\\rl-thesis\\src\\envs\\two_player_briscola\\TwoPlayerBriscola.py:162: UserWarning: Tried to execute an illegal action, executing 0 instead\n",
      "  warn(f\"Tried to execute an illegal action, executing {action} instead\")\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import play_all_moves_of_players\n",
    "\n",
    "device = \"cpu\"\n",
    "observation_shape = vec_env.single_observation_space()[\"observation\"].shape\n",
    "action_size = vec_env.single_action_space().n\n",
    "\n",
    "player_policy = NNAgent(observation_shape, action_size).to(device)\n",
    "\n",
    "player = vec_env[0].agents[0]\n",
    "other_player = vec_env[0].agents[1]\n",
    "pool = AgentPool(params[\"max_pool_size\"], nu=params[\"nu\"])\n",
    "\n",
    "optimizer = optim.Adam(player_policy.parameters(), lr=params[\"lr\"], eps=1e-5)\n",
    "\n",
    "obs = torch.zeros((params[\"n_steps\"], params[\"n_envs\"]) + observation_shape).to(device)\n",
    "actions = torch.zeros((params[\"n_steps\"], params[\"n_envs\"]), dtype=torch.int64).to(device)\n",
    "actions_masks = torch.zeros((params[\"n_steps\"], params[\"n_envs\"]) + (action_size,), dtype=torch.int64).to(device)\n",
    "logprobs = torch.zeros((params[\"n_steps\"], params[\"n_envs\"])).to(device)\n",
    "rewards = torch.zeros((params[\"n_steps\"], params[\"n_envs\"])).to(device)\n",
    "dones = torch.zeros((params[\"n_steps\"], params[\"n_envs\"]), dtype=torch.int8).to(device)\n",
    "values = torch.zeros((params[\"n_steps\"], params[\"n_envs\"])).to(device)\n",
    "\n",
    "global_step = 0\n",
    "start_time = time()\n",
    "for update in range(1, params[\"num_updates\"] + 1):\n",
    "    # Decay lr\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    optimizer.param_groups[0][\"lr\"] = max(params[\"lr_min\"], current_lr*params[\"lr_decay\"])\n",
    "\n",
    "    # Add agent to pool\n",
    "    if update % params[\"add_model_every_x_step\"] == 0:\n",
    "        pool.add_agent(deepcopy(player_policy))\n",
    "\n",
    "    # Sample agents\n",
    "    other_player_policies, opponent_indexes = pool.sample_agents(params[\"n_opponents\"] - params[\"self_play_opponents\"])\n",
    "    other_player_policies += [player_policy] * params[\"self_play_opponents\"]\n",
    "\n",
    "    # Play episodes\n",
    "    vec_env.reset()\n",
    "    play_all_moves_of_players(vec_env, other_player_policies, other_player)\n",
    "    next_obs, action_mask, reward, next_done = get_state_representation(vec_env)\n",
    "    for step in range(params[\"n_steps\"]):\n",
    "        global_step += params[\"n_envs\"]\n",
    "\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = player_policy.get_action_and_value(next_obs.to(device), action_mask.to(device))\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        actions_masks[step] = action_mask.to(device)\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        vec_env.step(actions[step].cpu().numpy())\n",
    "        play_all_moves_of_players(vec_env, other_player_policies, other_player)\n",
    "        next_obs, action_mask, reward, next_done = get_state_representation(vec_env)\n",
    "        wins = torch.tensor([env.game_winner() == player for env in vec_env], dtype=torch.float32)\n",
    "        rewards[step] = (1 - params[\"ratio_win_reward\"]) * reward + (params[\"ratio_win_reward\"] * next_done * wins).to(device)\n",
    "\n",
    "    # Update rating\n",
    "    scores = [env.get_game_outcome(other_player) for env in vec_env.get_envs()]\n",
    "    mean_score_per_opponent = np.empty_like(opponent_indexes, dtype=np.float64)\n",
    "    for i in range(opponent_indexes.size):\n",
    "        start, end = (i * len(scores)) // opponent_indexes.size, ((i + 1) * len(scores)) // opponent_indexes.size\n",
    "        mean_score_per_opponent[i] = np.mean(scores[start:end])\n",
    "\n",
    "    agent_rating = pool.update_ratings(0., mean_score_per_opponent, opponent_indexes)\n",
    "\n",
    "    # Bootstrap value\n",
    "    with torch.no_grad():\n",
    "        next_value = player_policy.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        last_gae_lambda = 0\n",
    "\n",
    "        for t in reversed(range(params[\"n_steps\"])):\n",
    "            if t == params[\"n_steps\"] - 1:\n",
    "                next_non_terminal = 1. - next_done\n",
    "                next_values = next_value\n",
    "            else:\n",
    "                next_non_terminal = 1. - dones[t+1]\n",
    "                next_values = values[t+1]\n",
    "\n",
    "            delta = rewards[t] + params[\"gamma\"] * next_values * next_non_terminal - values[t]\n",
    "            last_gae_lambda = delta + params[\"gamma\"] * params[\"lambda\"] * next_non_terminal * last_gae_lambda\n",
    "            advantages[t] = last_gae_lambda\n",
    "\n",
    "        returns = advantages + values\n",
    "\n",
    "    # Optimize net\n",
    "    b_obs = obs.reshape((-1,) + observation_shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape(-1)\n",
    "    b_action_masks = actions_masks.reshape((-1, action_size))\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    clip_fraction = []\n",
    "    b_indexes = np.arange(params[\"batch_size\"])\n",
    "    for epoch in range(params[\"update_epochs\"]):\n",
    "        np.random.shuffle(b_indexes)\n",
    "        for start in range(0, params[\"batch_size\"], params[\"mini_batch_size\"]):\n",
    "            end = start + params[\"mini_batch_size\"]\n",
    "            mb_indexes = b_indexes[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = player_policy.get_action_and_value(b_obs[mb_indexes], b_action_masks[mb_indexes], b_actions[mb_indexes])\n",
    "            logratio = newlogprob - b_logprobs[mb_indexes]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl https://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clip_fraction.append(((ratio - 1.0).abs() > params[\"clip_coef\"]).float().mean().item())\n",
    "\n",
    "            mb_advantages = b_advantages[mb_indexes]\n",
    "            if params[\"normalize_advantage\"]:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1-params[\"clip_coef\"], 1+params[\"clip_coef\"])\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if params[\"clip_value_loss\"]:\n",
    "                v_loss_unclipped = (newvalue - b_returns[mb_indexes]) ** 2\n",
    "                v_clipped = b_values[mb_indexes] + torch.clamp(\n",
    "                    newvalue - b_values[mb_indexes],\n",
    "                    -params[\"clip_coef\"],\n",
    "                    params[\"clip_coef\"],\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[mb_indexes]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                value_loss = 0.5 * v_loss_max.mean()\n",
    "            else:\n",
    "                value_loss = 0.5 * ((newvalue - b_returns[mb_indexes]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - params[\"entropy_coef\"] * entropy_loss + params[\"value_coef\"] * value_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(player_policy.parameters(), params[\"max_grad_norm\"])\n",
    "            optimizer.step()\n",
    "\n",
    "    # Logging\n",
    "    explained_var = explained_variance_score(b_returns.cpu().numpy(), b_values.cpu().numpy())\n",
    "    if (update - 1) % 8 == 0:\n",
    "        outcome_vs_random, rating_vs_random = compute_rating(player_policy, RandomAgent(action_size))\n",
    "\n",
    "    wandb.log({\n",
    "        \"global_step\": global_step,\n",
    "        \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "        \"value_loss\": value_loss.item(),\n",
    "        \"policy_loss\": pg_loss.item(),\n",
    "        \"entropy\": entropy_loss.item(),\n",
    "        \"total_loss\": loss.item(),\n",
    "        \"old_approx_kl\": old_approx_kl.item(),\n",
    "        \"approx_kl\": approx_kl.item(),\n",
    "        \"clipfrac\": np.mean(clip_fraction),\n",
    "        \"explained_variance\": explained_var,\n",
    "        \"SPS\": int(global_step / (time() - start_time)),\n",
    "        \"reward_per_game\": torch.sum(rewards, dim=0).mean(),\n",
    "        \"points_per_game\": sum([env.game_state.agent_points[player] for env in vec_env]) / params[\"n_envs\"],\n",
    "        \"ratio_game_won\": sum([env.game_winner() == player for env in vec_env]) / params[\"n_envs\"],\n",
    "        \"mean_outcome\": sum([env.get_game_outcome(player) for env in vec_env]) / params[\"n_envs\"],\n",
    "        \"outcome_vs_random\": outcome_vs_random,\n",
    "        \"rating_vs_random\": rating_vs_random,\n",
    "        \"pool_ratings\": wandb.Histogram(pool.ratings),\n",
    "        \"pool_entropy\": stats.entropy(pool.get_sampling_probability()),\n",
    "        \"pool_std\": np.std(pool.ratings),\n",
    "        \"pool_size\": len(pool)\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>SPS</td><td>▁▆▇█▇▅▅▆▆▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▅▅▅▅</td></tr><tr><td>approx_kl</td><td>▆▇█▇▇██▆▆▅▄▅▆▃▃▃▃▃▄▃▄▃▃▂▃▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁</td></tr><tr><td>clipfrac</td><td>▅▇█▇▇▆▇▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>entropy</td><td>█▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>explained_variance</td><td>▁███████████████████████████████████████</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>██▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_outcome</td><td>▁▃▄██▄▇▅▇▅▄▃▂▃▂▂▃▃▃▂▂▂▂▃▂▁▂▂▃▂▂▂▃▁▂▂▂▂▂▂</td></tr><tr><td>old_approx_kl</td><td>█▅▆█▇▆▇▄▅▅▄▃▄▅▅▃▂▃▄▃▃▄▂▃▃▃▂▂▃▂▂▂▂▃▁▁▂▂▁▁</td></tr><tr><td>outcome_vs_random</td><td>▁▁▄▅▆▆▆▇▆▇▇▇▇▇▇▇██▇█▇▇▇▇▇▇▇▇▇██▇▇█▇█▇█▇█</td></tr><tr><td>points_per_game</td><td>▁▄▄██▄▇▅▇▄▄▃▂▃▃▂▂▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂</td></tr><tr><td>policy_loss</td><td>▅▃▄▁▃▄▃▃▄▃▃▃▃▃▄▂▅▆▄▅▄▅▆▅▆▅▅▇▅▇▆▇█▇██▇▇▇▇</td></tr><tr><td>pool_entropy</td><td>▁▄▅▆▆▆▇▇▇███████████████████████████████</td></tr><tr><td>pool_size</td><td>▁▂▂▃▄▄▅▆▆▇██████████████████████████████</td></tr><tr><td>pool_std</td><td>▁▃▅██████▇█▇▅▄▄▃▃▃▃▃▃▂▃▃▃▂▃▃▂▂▂▂▂▃▂▂▂▂▂▂</td></tr><tr><td>rating_vs_random</td><td>▁▁▃▄▅▆▅▆▅▆▆▆▆▇▆▇▇▇▆█▇▆▇▇▇▇▇▇▇█▇▆▇▇▇█▇█▇▇</td></tr><tr><td>ratio_game_won</td><td>▁▄▄█▇▄▆▅▇▄▄▃▂▂▂▂▂▃▃▂▂▂▂▂▁▁▂▂▂▂▂▂▃▁▂▁▁▂▁▁</td></tr><tr><td>reward_per_game</td><td>▁▄▄█▇▄▆▅▇▄▄▃▂▂▂▂▂▃▃▂▂▂▂▂▁▁▂▂▂▂▂▂▃▁▂▁▁▂▁▁</td></tr><tr><td>total_loss</td><td>▄▂▄▁▃▄▃▃▄▃▃▃▃▄▄▃▅▆▅▅▅▅▆▅▆▆▆▇▅▇▆▇█▇██▇▇▇▇</td></tr><tr><td>value_loss</td><td>█▂▂▁▁▁▂▁▁▂▂▃▂▂▂▂▂▁▂▂▁▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>SPS</td><td>4975</td></tr><tr><td>approx_kl</td><td>0.00361</td></tr><tr><td>clipfrac</td><td>0.0108</td></tr><tr><td>entropy</td><td>0.22512</td></tr><tr><td>explained_variance</td><td>0.55823</td></tr><tr><td>global_step</td><td>19988480</td></tr><tr><td>learning_rate</td><td>0.0003</td></tr><tr><td>mean_outcome</td><td>0.50903</td></tr><tr><td>old_approx_kl</td><td>0.00272</td></tr><tr><td>outcome_vs_random</td><td>0.88184</td></tr><tr><td>points_per_game</td><td>60.36133</td></tr><tr><td>policy_loss</td><td>-0.0087</td></tr><tr><td>pool_entropy</td><td>4.846</td></tr><tr><td>pool_size</td><td>128</td></tr><tr><td>pool_std</td><td>0.10958</td></tr><tr><td>rating_vs_random</td><td>2.00993</td></tr><tr><td>ratio_game_won</td><td>0.49365</td></tr><tr><td>reward_per_game</td><td>0.04937</td></tr><tr><td>total_loss</td><td>-0.01081</td></tr><tr><td>value_loss</td><td>0.00028</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">smaller lr decay</strong>: <a href=\"https://wandb.ai/lettera/briscolaBot/runs/2pymwcoy\" target=\"_blank\">https://wandb.ai/lettera/briscolaBot/runs/2pymwcoy</a><br/>Synced 7 W&B file(s), 0 media file(s), 5 artifact file(s) and 1 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230111_001920-2pymwcoy\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(player_policy.state_dict(), 'agent.pt')\n",
    "# Save as artifact for version control.\n",
    "artifact = wandb.Artifact('model', type='model')\n",
    "artifact.add_file('agent.pt')\n",
    "run.log_artifact(artifact)\n",
    "wandb.run.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
